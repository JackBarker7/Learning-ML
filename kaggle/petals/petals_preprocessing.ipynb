{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be run once, to convert from `.tfrec` files to PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import albumentations\n",
    "import torch\n",
    "import numpy as np\n",
    "import io\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing files using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = [192, 224, 331, 512][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_local = True # change if running on cloud\n",
    "if is_local:\n",
    "    train_path = f'kaggle\\\\input\\\\tpu-getting-started\\\\tfrecords-jpeg-{img_size}x{img_size}\\\\train\\\\*.tfrec'\n",
    "    val_path = f'kaggle\\\\input\\\\tpu-getting-started\\\\tfrecords-jpeg-{img_size}x{img_size}\\\\val\\\\*.tfrec'\n",
    "    test_path = f'kaggle\\\\input\\\\tpu-getting-started\\\\tfrecords-jpeg-{img_size}x{img_size}\\\\test\\\\*.tfrec'\n",
    "else:\n",
    "    train_path = f'/kaggle/input/tpu-getting-started/tfrecords-jpeg-{img_size}x{img_size}/train/*.tfrec'\n",
    "    val_path = f'/kaggle/input/tpu-getting-started/tfrecords-jpeg-{img_size}x{img_size}/val/*.tfrec'\n",
    "    test_path = f'/kaggle/input/tpu-getting-started/tfrecords-jpeg-{img_size}x{img_size}/test/*.tfrec'\n",
    "\n",
    "train_files = glob.glob(train_path)\n",
    "val_files = glob.glob(val_path)\n",
    "test_files = glob.glob(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    'class': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'id': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image': tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "def _parse_image_function(example_proto):\n",
    "    return tf.io.parse_single_example(example_proto, feature_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and creating the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerDataset(Dataset):\n",
    "    def __init__(self, id, classes, image, img_height, img_width, mean, std):\n",
    "        self.id = id\n",
    "        self.classes = classes\n",
    "        self.image = image\n",
    "        \n",
    "        self.aug = albumentations.Compose([\n",
    "            albumentations.Resize(img_height, img_width),\n",
    "            albumentations.Normalize(mean, std, always_apply=True)\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.id)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        id = self.id[index]\n",
    "        img = np.array(Image.open(io.BytesIO(self.image[index])))\n",
    "        img = cv2.resize(img, dsize=(128, 128), interpolation=cv2.INTER_CUBIC)\n",
    "        img = self.aug(image=img)[\"image\"]\n",
    "        img = np.transpose(img, (2,0,1)).astype(np.float32)\n",
    "\n",
    "        return torch.tensor(img, dtype=torch.float), int(self.classes[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = []\n",
    "train_class = []\n",
    "train_images = []\n",
    "for i in train_files:\n",
    "    train_image_dataset = tf.data.TFRecordDataset(i)\n",
    "    train_image_dataset = train_image_dataset.map(_parse_image_function)\n",
    "    ids = [str(id_features['id'].numpy())[2:-1] for id_features in train_image_dataset] # [2:-1] is done to remove b' from 1st and 'from last in train id names\n",
    "    train_ids = train_ids + ids\n",
    "    classes = [int(class_features['class'].numpy()) for class_features in train_image_dataset]\n",
    "    train_class = train_class + classes\n",
    "    images = [image_features['image'].numpy() for image_features in train_image_dataset]\n",
    "    train_images = train_images + images\n",
    "\n",
    "val_ids = []\n",
    "val_class = []\n",
    "val_images = []\n",
    "for i in val_files:\n",
    "    val_image_dataset = tf.data.TFRecordDataset(i)\n",
    "    val_image_dataset = val_image_dataset.map(_parse_image_function)\n",
    "    ids = [str(id_features['id'].numpy())[2:-1] for id_features in val_image_dataset] # [2:-1] is done to remove b' from 1st and 'from last in val id names\n",
    "    val_ids = val_ids + ids\n",
    "    classes = [int(class_features['class'].numpy()) for class_features in val_image_dataset]\n",
    "    val_class = val_class + classes\n",
    "    images = [image_features['image'].numpy() for image_features in val_image_dataset]\n",
    "    val_images = val_images + images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = FlowerDataset(\n",
    "    id = train_ids, \n",
    "    classes = train_class, \n",
    "    image = train_images,\n",
    "    img_height = img_size,\n",
    "    img_width=img_size,\n",
    "    mean = (0.485, 0.456, 0.406),\n",
    "    std = (0.229, 0.224, 0.225),\n",
    ")\n",
    "vd = FlowerDataset(\n",
    "    id = val_ids, \n",
    "    classes = val_class, \n",
    "    image = val_images,\n",
    "    img_height = img_size,\n",
    "    img_width=img_size,\n",
    "    mean = (0.485, 0.456, 0.406),\n",
    "    std = (0.229, 0.224, 0.225),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(td, f=f\"train_dataset_{img_size}.pt\")\n",
    "torch.save(vd, f=f\"val_dataset_{img_size}.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
